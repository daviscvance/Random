{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pybullet_ann.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daviscvance/Random/blob/master/pybullet_ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KEh7Oj2mh7L",
        "colab_type": "code",
        "outputId": "b68a8a58-bdf3-47ef-f168-d83f84a0cc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# Building wheels will take a couple minutes! Make sure to use the GPU instance\n",
        "!pip install pybullet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/51/1417df166e39c7c7caef6582d3695136008e2e0420da38088289686635cf/pybullet-2.5.0.tar.gz (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 1.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/cf/c0/94af8f5460286129b176b5c3814156c0cfb4d18a47d9cc4d6b\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH0L5tglpAet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get -qq -y install ffmpeg > /dev/null\n",
        "# !ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBk03tTBoIN8",
        "colab_type": "code",
        "outputId": "440b71a7-8bc6-4f79-db7a-773c112c2556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1090
        }
      },
      "source": [
        "import random, numpy, math, time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import display, HTML\n",
        "import pybullet as pb\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import LSTM\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "\n",
        "####################\n",
        "# Environment (game)\n",
        "####################\n",
        "\n",
        "MAX_STEPS = 1000        # maximum number of simulation steps\n",
        "STEPS_AFTER_TARGET = 30 # number of simulation steps after reaching the goal\n",
        "TARGET_DELTA = 0.2      # value of acceptable rolling near the target (absolute value)\n",
        "FORCE_DELTA = 0.1       # step change of force (absolute value)\n",
        "PB_BallMass = 1         # ball weight\n",
        "PB_BallRadius = 0.2     # radius of the ball\n",
        "PB_HEIGHT = 10          # maximum height of raising the ball\n",
        "MAX_FORCE = 20          # maximum vertical force cut to the ball\n",
        "MIN_FORCE = 0           # minimum force sawn to the ball\n",
        "MAX_VEL = 14.2          # maximum vertical speed of the ball\n",
        "MIN_VEL = -14.2         # minimum vertical speed of the ball\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        # current state of environment\n",
        "        self.pb_z = 0               # current ball height\n",
        "        self.pb_force = 0           # current force applied to the ball\n",
        "        self.pb_velocity = 0        # current vertical speed of the ball\n",
        "        self.z_target = 0           # target height\n",
        "        self.start_time = 0         # start time of the new game\n",
        "        self.steps = 0              # number of steps after the start of the simulation\n",
        "        self.target_area = 0        # fact of reaching the target\n",
        "        self.steps_after_target = 0 # number of steps after reaching the goal\n",
        " \n",
        "        # create a simulation\n",
        "        self.pb_physicsClient = pb.connect(pb.DIRECT)\n",
        "\n",
        "    def reset(self):\n",
        "        # random height of the ball and target height\n",
        "        z_target = random.uniform(0.01, 0.99)\n",
        "        self.z_target = PB_BallRadius + z_target*PB_HEIGHT\n",
        "        z = random.uniform(0.05, 0.95)\n",
        "        self.pb_z = PB_BallRadius + z*PB_HEIGHT\n",
        "        \n",
        "        # reset of environmental parameters\n",
        "        pb.resetSimulation()\n",
        "        self.target_area = 0\n",
        "        self.start_time = time.time()\n",
        "        self.steps = 0\n",
        "        self.steps_after_target = 0\n",
        "        \n",
        "        # simulation step 1/60 sec..\n",
        "        pb.setTimeStep(1./60)\n",
        "        \n",
        "        # surface\n",
        "        floorColShape = pb.createCollisionShape(pb.GEOM_PLANE)\n",
        "        # for GEOM_PLANE, visualShape - not displayed, we will use GEOM_BOX\n",
        "        floorVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[100,100,0.0001], rgbaColor=[1,1,.98,1])\n",
        "        self.pb_floorId = pb.createMultiBody(0,floorColShape,floorVisualShapeId, [0,0,0], [0,0,0,1])# (mass,collisionShape,visualShape)\n",
        "        \n",
        "        # orb\n",
        "        ballPosition = [0,0,self.pb_z]\n",
        "        ballOrientation=[0,0,0,1]\n",
        "        ballColShape = pb.createCollisionShape(pb.GEOM_SPHERE,radius=PB_BallRadius)\n",
        "        ballVisualShapeId = pb.createVisualShape(pb.GEOM_SPHERE,radius=PB_BallRadius, rgbaColor=[0.25, 0.75, 0.25,1])\n",
        "        self.pb_ballId = pb.createMultiBody(PB_BallMass, ballColShape, ballVisualShapeId, ballPosition, ballOrientation) #(mass, collisionShape, visualShape, ballPosition, ballOrientation)\n",
        "        #pb.changeVisualShape(self.pb_ballId,-1,rgbaColor=[1,0.27,0,1])\n",
        "        \n",
        "        # target pointer (without CollisionShape, only display (VisualShape))\n",
        "        targetPosition = [0,0,self.z_target]\n",
        "        targetOrientation=[0,0,0,1]\n",
        "        targetVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[1,0.025,0.025], rgbaColor=[0,0,0,1])\n",
        "        self.pb_targetId = pb.createMultiBody(0,-1, targetVisualShapeId, targetPosition, targetOrientation)\n",
        "\n",
        "        # gravity\n",
        "        pb.setGravity(0,0,-10)\n",
        "\n",
        "        # limit the motion of the ball only along the vertical axes\n",
        "        pb.createConstraint(self.pb_floorId, -1, self.pb_ballId, -1, pb.JOINT_PRISMATIC, [0,0,1], [0,0,0], [0,0,0])\n",
        "\n",
        "        # set the acting force on the ball to compensate for gravity\n",
        "        self.pb_force = 10 * PB_BallMass\n",
        "        pb.applyExternalForce(self.pb_ballId, -1, [0,0,self.pb_force], [0,0,0], pb.LINK_FRAME)\n",
        "                \n",
        "        # return values\n",
        "        observation = self.getObservation()\n",
        "        reward, done = self.getReward()\n",
        "        info = self.getInfo()\n",
        "        return [observation, reward, done, info]\n",
        "\n",
        "    # Observations (return normalized)\n",
        "    def getObservation(self):\n",
        "        # distance to target\n",
        "        d_target =  0.5 + (self.pb_z - self.z_target)/(2*PB_HEIGHT)\n",
        "        # acting force\n",
        "        force = (self.pb_force-MIN_FORCE)/(MAX_FORCE-MIN_FORCE)\n",
        "        # current ball height\n",
        "        z = (self.pb_z-PB_BallRadius)/PB_HEIGHT\n",
        "        # current speed\n",
        "        z_velocity = (self.pb_velocity-MIN_VEL)/(MAX_VEL-MIN_VEL)\n",
        "        state = [d_target, force, z_velocity]\n",
        "        return state\n",
        "\n",
        "    # вычисление награды за действие\n",
        "    def getReward(self):\n",
        "        done = False\n",
        "        z_reward = 0\n",
        "        # Факт достижения цели, после чего ждем STEPS_AFTER_TARGET шагов и завершем игру.\n",
        "        if (TARGET_DELTA >= math.fabs(self.z_target - self.pb_z)):\n",
        "            self.target_area = 1\n",
        "            z_reward = 1\n",
        "        # Выход за пределы зоны\n",
        "        if (self.pb_z > (PB_HEIGHT + PB_BallRadius) or self.pb_z < PB_BallRadius):\n",
        "            done = True\n",
        "        # Завершение игры после достижения цели\n",
        "        if (self.target_area > 0):\n",
        "            self.steps_after_target += 1\n",
        "            if (self.steps_after_target>=STEPS_AFTER_TARGET):\n",
        "                done = True\n",
        "        # Завершение игры по таймауту\n",
        "        if (self.steps >= MAX_STEPS):\n",
        "            done = True\n",
        "\n",
        "        return [z_reward, done]\n",
        "    \n",
        "    # Дополнительная информация для сбора статистики\n",
        "    def getInfo(self):\n",
        "        game_time = time.time() - self.start_time\n",
        "        if game_time:\n",
        "            fps = round(self.steps/game_time)\n",
        "        return {'step': self.steps, 'fps': fps}\n",
        "\n",
        "    # Запуск шага симуляции согласно переданному действию\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        if action == 0:\n",
        "            # 0 - увеличение приложеной силы\n",
        "            self.pb_force -= FORCE_DELTA\n",
        "            if self.pb_force < MIN_FORCE:\n",
        "                self.pb_force = MIN_FORCE\n",
        "        else:\n",
        "            # 1 - уменьшение приложенной силы\n",
        "            self.pb_force += FORCE_DELTA\n",
        "            if self.pb_force > MAX_FORCE:\n",
        "                self.pb_force = MAX_FORCE\n",
        "        \n",
        "        # изменим текущую сил и запустим шаг симуляции\n",
        "        pb.applyExternalForce(self.pb_ballId, -1, [0,0,self.pb_force], [0,0,0], pb.LINK_FRAME)\n",
        "        pb.stepSimulation()\n",
        "        \n",
        "        # обновим парамтры состояния окружения (положение и скорость шара)\n",
        "        curPos, curOrient = pb.getBasePositionAndOrientation(self.pb_ballId)\n",
        "        lin_vel, ang_vel= pb.getBaseVelocity(self.pb_ballId)\n",
        "        self.pb_z = curPos[2]\n",
        "        self.pb_velocity = lin_vel[2]\n",
        "        \n",
        "        # вернем наблюдения, награду, факт окончания игры и доп.информацию\n",
        "        observation = self.getObservation()\n",
        "        reward, done = self.getReward()\n",
        "        info = self.getInfo()\n",
        "        return [observation, reward, done, info]\n",
        "    \n",
        "    # Текущее изображение с камеры\n",
        "    def render(self):\n",
        "        camTargetPos = [0,0,5] # расположение цели (фокуса) камеры\n",
        "        camDistance = 10       # дистанция камеры от цели\n",
        "        yaw = 0                # угол рыскания относительно цели\n",
        "        pitch = 0              # наклон камеры относительно цели\n",
        "        roll=0                 # угол крена камеры относительно цели\n",
        "        upAxisIndex = 2        # ось вертикали камеры (z)\n",
        "\n",
        "        fov = 60               # угол зрения камеры\n",
        "        nearPlane = 0.01       # расстояние до ближней плоскости отсечения\n",
        "        farPlane = 20          # расстояние до дальной плоскости отсечения\n",
        "        pixelWidth = 320       # ширина изображения\n",
        "        pixelHeight = 200      # высота изображения\n",
        "        aspect = pixelWidth/pixelHeight;  # соотношение сторон изображения\n",
        "       \n",
        "        # видовая матрица\n",
        "        viewMatrix = pb.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)\n",
        "        # проекционная матрица\n",
        "        projectionMatrix = pb.computeProjectionMatrixFOV(fov, aspect, nearPlane, farPlane);\n",
        "        # рендеринг изображения с камеры\n",
        "        img_arr = pb.getCameraImage(pixelWidth, pixelHeight, viewMatrix, projectionMatrix, shadow=0, lightDirection=[0,1,1],renderer=pb.ER_TINY_RENDERER)\n",
        "        w=img_arr[0] #width of the image, in pixels\n",
        "        h=img_arr[1] #height of the image, in pixels\n",
        "        rgb=img_arr[2] #color data RGB\n",
        "        dep=img_arr[3] #depth data\n",
        "        \n",
        "        # вернем rgb матрицу\n",
        "        return rgb\n",
        "    \n",
        "#################################\n",
        "# Память для обучающих примеров\n",
        "#################################\n",
        "\n",
        "MEMORY_CAPACITY = 200000\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.samples = []   # хранятся кортежи типа ( s, a, r, s_ )\n",
        "\n",
        "    def add(self, sample):\n",
        "        self.samples.append(sample)\n",
        "        if len(self.samples) > MEMORY_CAPACITY:\n",
        "            self.samples.pop(0)\n",
        "\n",
        "    def sample(self, n):\n",
        "        n = min(n, len(self.samples))\n",
        "        return random.sample(self.samples, n)\n",
        "\n",
        "    \n",
        "##################\n",
        "# Нейронная сеть\n",
        "##################\n",
        "\n",
        "LAYER_SIZE = 512       # размер слоя\n",
        "STATE_CNT  = 3         # количество входных пераметров (расстояние до цели + действующая сила + скорость)\n",
        "ACTION_CNT = 2         # количесво выходов (награда за уменьшение и увеличение силы)\n",
        "class Brain:\n",
        "    def __init__(self):\n",
        "        self.model = self._QNetwork()\n",
        "        \n",
        "    def _QNetwork(self):\n",
        "        # Создадим сеть используя Keras\n",
        "        model = Sequential()\n",
        "        model.add(Dense(units=LAYER_SIZE, activation='relu', input_dim=STATE_CNT))\n",
        "        model.add(Dense(units=LAYER_SIZE, activation='relu'))\n",
        "        model.add(Dense(units=ACTION_CNT, activation='linear'))\n",
        "        opt = RMSprop(lr=0.00025)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "        return model\n",
        "    \n",
        "    # обучение по одному пакету обучающих примеров \n",
        "    def train(self, x, y, batch_size=32, epoch=1, verbose=0):\n",
        "        self.model.fit(x, y, batch_size=batch_size, epochs=epoch, verbose=verbose)\n",
        "\n",
        "    # предсказания сети по списку начальных состояний\n",
        "    def predict(self, s):\n",
        "        return self.model.predict(s)\n",
        "\n",
        "    #  предсказания сети по одном начальному состоянию\n",
        "    def predictOne(self, s):\n",
        "        s = numpy.array(s)\n",
        "        predictions = self.predict(s.reshape(1, STATE_CNT)).flatten()\n",
        "        return predictions\n",
        "\n",
        "###############\n",
        "# Агент\n",
        "###############\n",
        "\n",
        "GAMMA = 0.98        # фактор дисконтирования\n",
        "MAX_EPSILON = 0.5   # максимальная вероятность выбора случайного действия\n",
        "MIN_EPSILON = 0.1   # минимальная вероятность выбора случайного действия\n",
        "LAMBDA = 0.001      # параметр определяющий скорость уменьшения вероятности выбора случайного действия\n",
        "BATCH_SIZE = 32     # размер обучающего пакета\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.brain = Brain()                     # Нейронная сеть для обучения\n",
        "        self.memory = Memory()                   # Хранилище обучающих примеров\n",
        "        self.epsilon = MAX_EPSILON               # Определяет вероятность выбора случайного действия\n",
        "\n",
        "    # выбор действия\n",
        "    def act(self, s):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, ACTION_CNT - 1)        # выбираем случайное действие\n",
        "        else:\n",
        "            return numpy.argmax(self.brain.predictOne(s))   # выбираем оптимальное действие\n",
        "        \n",
        "    # изменение состояния агента\n",
        "    def observe(self, sample, game_num):  # sample = (s, a, r, s_)\n",
        "        self.memory.add(sample)\n",
        "        self.epsilon = MIN_EPSILON + (MAX_EPSILON-MIN_EPSILON)*math.exp(-LAMBDA*game_num)\n",
        "\n",
        "    # обучение по случайному пакету обучающих примеров (batch)\n",
        "    def train(self):\n",
        "        batch = self.memory.sample(BATCH_SIZE)\n",
        "        batchLen = len(batch)  \n",
        "        if batchLen<BATCH_SIZE: # будем обучаться только если есть достаточное количество примеров в памяти\n",
        "            return\n",
        "\n",
        "        # начальные состояния из пакета\n",
        "        states = numpy.array([ o[0] for o in batch ])\n",
        "        # начальные состояния из пакета\n",
        "        states_ = numpy.array([ o[3] for o in batch ])\n",
        "\n",
        "        # выгоды для начальных состояний\n",
        "        p = agent.brain.predict(states)\n",
        "        # выгоды для конечных состояний\n",
        "        p_ = agent.brain.predict(states_)\n",
        "\n",
        "        # сформируем пустой обучающий пакет\n",
        "        x = numpy.zeros((batchLen, STATE_CNT))\n",
        "        y = numpy.zeros((batchLen, ACTION_CNT))\n",
        "\n",
        "        # заполним пакет\n",
        "        for i in range(batchLen):\n",
        "            o = batch[i]\n",
        "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
        "\n",
        "            t = p[i] # выгоды действий для начального состояния\n",
        "            # обновим выгоду только для совершенного действия, для неиспользованных действий выгоды останутся прежними\n",
        "            t[a] = r + GAMMA * numpy.amax(p_[i]) # вычислим новую выгоду действия используя награду и максимальную выгоду конечного состояния\n",
        "            \n",
        "            # сохраним значения в batch\n",
        "            x[i] = s\n",
        "            y[i] = t\n",
        "\n",
        "        # обучим сеть по данному пакету\n",
        "        self.brain.train(x, y)\n",
        "\n",
        "#######################\n",
        "# Статистика\n",
        "#######################\n",
        "\n",
        "class Stats():\n",
        "    def __init__(self):\n",
        "        self.stats={\"game_num\": [],\"rewards\": [], \"success_steps\": [], \"fps\": [], \"steps\":[], \"epsilon\":[]}\n",
        "\n",
        "    def save_stat(self, R, info, epsilon, game_num):\n",
        "        self.stats[\"rewards\"].append(R)\n",
        "        self.stats[\"success_steps\"].append(R/STEPS_AFTER_TARGET)\n",
        "        self.stats[\"game_num\"].append(game_num)\n",
        "        self.stats[\"epsilon\"].append(epsilon)\n",
        "        self.stats[\"steps\"].append(info[\"step\"])\n",
        "        self.stats[\"fps\"].append(info[\"fps\"])\n",
        "    def show_stat(self):\n",
        "        # отобраим процент удачных шагов за опыт\n",
        "        plt.plot(self.stats[\"game_num\"], self.stats[\"success_steps\"], \"b.\")\n",
        "        # отобразим сглаженный график\n",
        "        x, y = self.fit_data(self.stats[\"game_num\"],  self.stats[\"success_steps\"])\n",
        "        plt.plot(x, y, \"r-\")\n",
        "        # второй вариант сглаживания    \n",
        "        # plt.plot(numpy.linspace(self.stats[\"game_num\"][0], self.stats[\"game_num\"][-1],50), numpy.average(numpy.array_split(self.stats[\"success_steps\"][:-1], 50),1), \"g-\")\n",
        "        plt.show()\n",
        "    #  Полиномиальное сглаживание\n",
        "    def fit_data(self, x, y):\n",
        "        z = numpy.polyfit(x, y, 3)\n",
        "        f = numpy.poly1d(z)\n",
        "        # новые данные размерностью 50\n",
        "        x_new = numpy.linspace(x[0], x[-1], 50)\n",
        "        y_new = f(x_new)\n",
        "        return [x_new, y_new]\n",
        "\n",
        "###########\n",
        "# MAIN\n",
        "###########\n",
        "%matplotlib inline\n",
        "MAX_GAMES = 50000   # максимальное количество игр\n",
        "RENDER_PERIOD = 100 # период генерации видео с опытом (0 для отключения)\n",
        "\n",
        "env = Environment()\n",
        "agent = Agent()\n",
        "stats = Stats()\n",
        "\n",
        "for game_num in range(MAX_GAMES):\n",
        "    print (\"Game %d:\" % game_num)\n",
        "    render_imgs = []\n",
        "    observation, r, done, info = env.reset()\n",
        "    s = observation\n",
        "    R = r\n",
        "    \n",
        "    if RENDER_PERIOD and (game_num % RENDER_PERIOD == 0):\n",
        "        plt.subplots()\n",
        "    \n",
        "    while True:\n",
        "        # возьмем оптимальное действие на основе текущего состояния\n",
        "        a = agent.act(s)\n",
        "        # запустим шаг симуляции\n",
        "        observation, r, done, info = env.step(a)\n",
        "        s_ = observation # новое состояние\n",
        "        # сохраним состояние агента\n",
        "        agent.observe((s, a, r, s_), game_num)\n",
        "        # обучим сеть по случайносу batch-у\n",
        "        agent.train()\n",
        "        \n",
        "        s = s_\n",
        "        R += r\n",
        "        \n",
        "        # сохраним изображение, если необходимо\n",
        "        if RENDER_PERIOD and game_num % RENDER_PERIOD == 0:\n",
        "            rgb = env.render()\n",
        "            render_imgs.append([plt.imshow(rgb, animated=True)])\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        #time.sleep(1./130)\n",
        "\n",
        "    print(\"Total reward:\", R, \" FPS:\", info['fps'])\n",
        "    \n",
        "    # сохраним статистику\n",
        "    stats.save_stat(R, info, agent.epsilon, game_num)\n",
        "    \n",
        "    # сформируем анимацию игры и графики статистики обучения\n",
        "    if len(render_imgs):\n",
        "        render_start = time.time()\n",
        "        ani = animation.ArtistAnimation(plt.gcf(), render_imgs, interval=10, blit=True,repeat_delay=1000)\n",
        "        plt.close()\n",
        "        display(HTML(ani.to_html5_video()))\n",
        "        # статистика\n",
        "        if game_num != 0:\n",
        "            plt.subplots(figsize=(10,4))\n",
        "            stats.show_stat()\n",
        "            plt.close()\n",
        "        render_stop = time.time()\n",
        "        print (\"render time: %f sec.\\n---\\n\" % (render_stop - render_start))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 02:26:45.790330 140444287817600 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Game 0:\n",
            "Total reward: 13  FPS: 23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video width=\"432\" height=\"288\" controls autoplay loop>\n",
              "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAgWW1kYXQAAAKuBgX//6rcRem9\n",
              "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTUyIHIyODU0IGU5YTU5MDMgLSBILjI2NC9NUEVHLTQg\n",
              "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE3IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
              "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
              "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
              "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
              "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MyBsb29r\n",
              "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
              "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
              "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
              "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
              "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
              "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAiBZYiE\n",
              "ACf//vWxfApqyfOKDOgyLuGXJMmutiLibQDAAAADAAVQSrRfODf9+M4AAPEn1LM4fWIBbCmzE8aY\n",
              "WAa72e6AjViJ/7C35Vo+dH1x1Q0txyfkqYWq6rK25rQxvKOtz8Mf0ATPqk+uwXJbujDO5IQoibfV\n",
              "KFfij1Dt1ACnvGcQypHcZIXX+YcpSlL5t6ftGYKVECcMkqTonmt+B3UCJXBwpaNWKCt7hEeYWDmV\n",
              "XI39+ipLuSeXIq+jQcCfpy+027TTe089I4IABNuEc0vTUsP1SO0bV6h5VhIRPMCDwD8PCO/kqZBl\n",
              "S4OBbEH2Qv4ra+LNF5BYGcfLM/kgip4EKHNKbW1nokRU6wCgs/WCc0Npft1zDFCOsqJY6iAAAASm\n",
              "GmYn3Kge0YCeMXz8+xz9ow8V1WhiupeRUilUqvfR7uoF4reCdAdLMjH3ijP/rA4sHMK/y4V1dnNd\n",
              "gdAfS+ZUjRu+UO3I5s7bqhIxXrg7SeO2C2gbACSkZ7gRcEG0LjK5iS32EPiCrRzlaA7GGgAAPMaX\n",
              "yygr1docIyYMIG9X9rd71h6aWQx9UQE6ArY1j+kiYGtXiVshfzC+2os01qDP93d0vGtlsUTWyQ4R\n",
              "9v8+a9RDv+0YAWP4uhEMJ1LVliPkTcNeTi+G8tx4hi7l8Gzuo/ep2EmxmNK1GGgb/lxDC3hoCBcJ\n",
              "HrKDrrU17ACJ65HW29dXhcWMYOHVBrLQnqi3EjaIcSS4XtUeWWXxHnCmXDgWw3lf8GGN+EZAwnjk\n",
              "dv5vqHaIaYh+zauzv0WyOhE2xnL/npDYtvRUnpzpgew6CNT6CocunFr4Y7H97mSaZuIk4buZgsqq\n",
              "IXrdwGHBw9ABiWI2DWfgPPcHcydbkmQ4QCb/tO1RuPGuRD2A6aib7JCa3W8SVgysN2bADfzmc4/X\n",
              "6fqA7h6EyoiRZe+vEXMWUy0atNaQg/y/O1tj3yUJRkgQJhEx/RAjER08gh4lLNKLKiHU/XaLwDDS\n",
              "xgEAWcW3cLZ/ANseTQNFr3S1nz691RjWMoR5qgYSWIrLhW7oAQO2gXmKc0fD14eP5l6jdfi2Y3PP\n",
              "VlHwX4JFCOV308zvk1/y8gbNmAVz5v9ZyP5H15OS5M7l58lfBHjedPMDZuh0R1Z/JaIedYLLwYjd\n",
              "TmE2OnTzS3+AAd7weCSUZ6sAxde+iX96T/nyStgPzLpANDTlfV+g+F0hXT6Xe2zt01ae9v2OwkSO\n",
              "vGp0HzKDzKAaClJUYPt+oFUv6PUqRy2AgMNiGnr44rwVU/wGsclWvIrdQQ7G85hZEHNJcOsnNptl\n",
              "WvrPzAxCRGhIH2igI7LyQQhxnBTtoifKxTxd1ljchc8w42a4yFvh5iWxNnSDR13EHm8NtXiRLT6Z\n",
              "7OPJbEKBAAyfWhXCrtuxh0W/M465R8hH63vj49Ow7rtzDXoXwbN1Eu4VyncrtZCv4Xn++FCFZ2X7\n",
              "d/bUXWmFrV+sfdldRtBDYVjb6n6CmvvW3zGgejUjIg1jFMcjs2K/OWmyc9T8+8qMyq5A/PTaOjbK\n",
              "j3uYOoEshtfQ+67kJ4M3YxjA/nTI7yTloJRh5sXGGB0BDHDwi1jHnr8PoK0h4B6xLfz4NpbicLHi\n",
              "nLEBWOQF1QAptkrh7tqnwKGWQc/rc8oRtDVRjxVI3hsKr6Bxbjf9B973pvgQcF1INzP4nCfIInJ+\n",
              "VOjQDX0hzlrk1uRfEYCQ7Ju8/RNEikciar36f/57MQ85CaqzUNQskMdCqYKi33R52oBwl43ri5tv\n",
              "yIKgvSK18AIaD5jhlBt1sP/m/uBNnFtHtBLpsH63FopnMYwveGWKMimC0SBB4ghoGF7GK1bGc0bp\n",
              "l65MeMabccHEd5VeZYD0Q/i4S9cf30iQ1XhxulQYhyW+JhzBO1ZlKPIw8UbGWRwsTZbWuagUrtUC\n",
              "/tCMsVg7PgXNN3++VdUwx3PiMaWz3s5v046tS7WgdWlbZ3BRVfJBU93TSvKYu0phiGPe4pws6h7/\n",
              "vfpSDSQDMHOZlWcqszbbf7qqVX9aOCx0J6Dyqp3+lM2UAQF8qgm2JQan8rezwV+CLYfXtQ1hsAuu\n",
              "s70ZHJB9+anlx1utm2kXiKsAsrBECmpiLHPdSNjoMeFlrYlKP69tfhjc7ztC9T48fHDEDM3KEcQ2\n",
              "833xUGskAU6shFScxblKlhg9PzKh91v7GFzGNcxVxEXo3+QRtdhOqr5p3KmusiniwykZ63g1U+hx\n",
              "CeZb6XR+TpusximD0jK5DNd1lWZaggqTO9Y6KWTkswXz71jqLerGpvCkdxcadesBXN6d/hLeNWb7\n",
              "kQVOet8FnvyqAEzWGDE42Lv2JumR/YTLnId3jBODS/h6cJ3ULrvCosakk0At/8e1OH1f3cWtKpYh\n",
              "ojWV0Y+ng7rucUNes7Mdmm7GnEBOFQN4tyJpiqCMDN+YmN4//0ql6dKQUxO1L0uSmNnjbEW+2Ag0\n",
              "Jk5pBKa2V5gASfdAqz+ukB3VtuNFOo1N9x5HC1pUSnnm1aMdFr7bHo0rvXv0XHPwbAedM2PX9e5D\n",
              "XP5tW/57gDp5wPSSWNwvyZsMNG90Irb1MjbZTmPM01btXqRUZxkPImqTTzJ//nwBWn58yrVv7+qW\n",
              "0KSLwAXbkeeoYHjdlblfzYBHm+SW7GJXTE9Iv8zjdHoyqwqJRDW76iESj2iI1tV5NRYr7qaDcGHV\n",
              "wczqy0x1dcQyCa4not03Ic1pqjkcodLYf1qwiCuSGtScKtuWohtyg6t3e5vP6/FCAsiHonG8D9bC\n",
              "myvoV27eosNd53BQIlAwFs3SJMzXgpLkQHa3pnKYrwaPMP8ZVtfF7QBCHZALdY0oTKpWneyvuTJF\n",
              "M1i7pf2zDSl9OnCl0dDcdOtx9yGkOU1zx277eAZbgmpV6O36AgEx1L68nbm95IHaSnPjzzl6yZFY\n",
              "LS3lPAEljZcAAABwQZokbEJ//fEASydWgDFNMJODlweLr1eO6XKgIndDFluQ1XIfTsw6uMvOqRI1\n",
              "iy5wRtI1IOcfow13nyz3CIs8fx8ja+t9okohPVWu/3Hd2lwbNI6SNdfazTQ+aqTCzAC9X7PRMsaE\n",
              "ZY4wuQWHCkY1IAAAABlBnkJ4jf8ANXVeaN51v+Q/Y86pUdIwMurBAAAAEwGeYXRF/wAhM5dY46jj\n",
              "89AhwHAAAAARAZ5jakX/AEKDJpRXnuyzYkkAAAA3QZpoSahBaJlMCE///fEAAZFcW4ArUXpw/YEp\n",
              "sxW9Mmb9WwByI0Z3yLFzDXrwwvwuy3wBgrbbQQAAABRBnoZFESxvADV1X2sYEcsP1UtsgQAAABAB\n",
              "nqV0Rf8AQ0lHjHC0nt8hAAAADQGep2pF/wBDfQuC4jYAAAAhQZqsSahBbJlMCE///fEAAZFcW4Aq\n",
              "sKneQM7VPW7pwFbAAAAAEUGeykUVLG8ANXVe7MAMkQ1pAAAADAGe6XRF/wBDSSrFTAAAAAwBnutq\n",
              "Rf8AQ30QxCwAAAAUQZrwSahBbJlMCE///fEAAAMARcEAAAAQQZ8ORRUsbwA1dV8SEtt8EQAAAAwB\n",
              "ny10Rf8AQ0kqxU0AAAAMAZ8vakX/AEN9EMQsAAAAMEGbNEmoQWyZTAhP//3xAAGRXFuAK3k1O1ia\n",
              "b5xI7ZSJypUbCYXcyIEAOEiibDMPaAAAABJBn1JFFSxvADV1Xuwuug16WtEAAAAOAZ9xdEX/AENJ\n",
              "ICKpMYEAAAAMAZ9zakX/AEN9EMQsAAAAMkGbeEmoQWyZTAhP//3xAAGRmXbgCleeO7ZBs4SLLGP1\n",
              "LY1wxGYs/w2rfKSG/nEUbxeRAAAAEkGflkUVLG8ANXVdiX9DEW6sygAAAA4Bn7V0Rf8AQ0lHb/+K\n",
              "2QAAAA0Bn7dqRf8AQ30HDcRtAAAAIEGbvEmoQWyZTAhP//3xAAGRXFuAOX9KdVLNp9RSXAeMAAAA\n",
              "EkGf2kUVLG8ANh92RYiEgWGs/wAAAA0Bn/l0Rf8AQ0kbLMRsAAAADAGf+2pF/wBDfRDELQAAABRB\n",
              "m+BJqEFsmUwIT//98QAAAwBFwQAAABBBnh5FFSxvADYfzBDe34KAAAAADAGePXRF/wBDSSrFTAAA\n",
              "AAwBnj9qRf8AQ30QxC0AAAApQZokSahBbJlMCE///fEAAZFcW4ApkjxEoNyN7r4aPP5mOqZszJOI\n",
              "akAAAAAQQZ5CRRUsbwA2H8wQ3t+CgQAAAA0BnmF0Rf8AQ0klPcRsAAAADQGeY2pF/wBDfQcNxG0A\n",
              "AAA0QZpoSahBbJlMCE///fEAAZGZduAKWi9OKIYQctZdcvkNSsH7dFZMLBc5kdKNsDuj9bmGpQAA\n",
              "ABxBnoZFFSxvADYfy/tkAFssh+EWOq75n0/HxXu5AAAADQGepXRF/wAhpCByxI0AAAANAZ6nakX/\n",
              "ACG+NrKFtAAAABlBmqxJqEFsmUwIT//98QADJ+m4P5yjMHtAAAAAEEGeykUVLG8ANfuF5gVy42kA\n",
              "AAANAZ7pdEX/AENIQOWEjAAAAAoBnutqRf8AAAYEAAAAMUGa8EmoQWyZTAhP//3xAAGTK6ZBoAVI\n",
              "koTEkpW9F6Tz46HTSCzPsoNi8kVKNfcbxeUAAAAaQZ8ORRUsbwAbHLkACyyO/8RtAB9XKePjRxcA\n",
              "AAAMAZ8tdEX/AEGH1MZVAAAACgGfL2pF/wAABgQAAAAYQZs0SahBbJlMCE///fEAAZGtiJoFhvF5\n",
              "AAAAF0GfUkUVLG8AGq8AzqABNE40u8kiW39DAAAADgGfcXRF/wAhYQ6eyxIwAAAADQGfc2pF/wAh\n",
              "qvgrEjAAAAAYQZt4SahBbJlMCE///fEAAZGtiJoFhvF5AAAAGEGflkUVLG8AGwoPIAEyCnyHLsWQ\n",
              "oV4RwwAAAA0Bn7V0Rf8AQ4fOFKWVAAAADgGft2pF/wBDV6MZX4rZAAAAGEGbvEmoQWyZTAhP//3x\n",
              "AAGRrYiaBYbxeQAAABhBn9pFFSxvABqvAM6gAWuR4HlbBT6Cp6EAAAAOAZ/5dEX/ACFhDp7LEjAA\n",
              "AAANAZ/7akX/ACGq+CsSMQAAACNBm+BJqEFsmUwIT//98QABkk3sSB7ngDjW+8s+s5GH4LkTcQAA\n",
              "ABpBnh5FFSxvABsNFdKQALgPHjpMRmMKFeBfsAAAAAoBnj10Rf8AAAYEAAAAFQGeP2pF/wAhwFnA\n",
              "Am7ok1k4cNKhbQAAACdBmiRJqEFsmUwIT//98QABkaFnnQBGZbJr4yC0oq7mAnyi/lhuGVAAAAAS\n",
              "QZ5CRRUsbwAbDRU0WLQrwPJBAAAADQGeYXRF/wAhpCByxIwAAAAMAZ5jakX/ACC+iGRNAAAAHEGa\n",
              "aEmoQWyZTAhP//3xAAGRoWedAEUu3uLkTcEAAAARQZ6GRRUsbwAbDRUuUMFeFUcAAAAKAZ6ldEX/\n",
              "AAAGBQAAAAwBnqdqRf8AIau6CFgAAAAbQZqsSahBbJlMCE///fEAAZF8CjYmSEptcMqAAAAAG0Ge\n",
              "ykUVLG8AGxDjpSABOiSOPO9x1+uTx8bIbQAAAAwBnul0Rf8AIMPqZlQAAAAMAZ7rakX/ACCrugiY\n",
              "AAAAGUGa8EmoQWyZTAhP//3xAAGRrWitAsN4vIEAAAAiQZ8ORRUsbwAbEOwOgAW4l/7bRsf+VYj0\n",
              "zbcXv9qwVyO/QQAAAA8Bny10Rf8AIZZr+COllZUAAAAMAZ8vakX/ABDV3QhYAAAAL0GbNEmoQWyZ\n",
              "TAhP//3xAAGRfCIgALDk1O8kjvrd7yXFhSYlDuCxK+/6GuOtoyHgAAAAEUGfUkUVLG8AGxDieDLQ\n",
              "ruPJAAAADAGfcXRF/wAhw+pmLAAAAA0Bn3NqRf8AIYRMEELAAAAAIkGbeEmoQWyZTAhP//3xAAGR\n",
              "fB50AX4FlI1qbuvxLgjDSbkAAAAcQZ+WRRUsbwAa5WxungAfgx91fuZnEzrRLeCwQAAAAAwBn7V0\n",
              "Rf8AIKSVZW0AAAAeAZ+3akX/ACG/gCAEyCMhC+o1amQAnLXbo1d+XULbAAAAK0GbvEmoQWyZTAhP\n",
              "//3xAAGRfArdPZ8AIc5miUopag3qjEf8QeaURjsNJuAAAAATQZ/aRRUsbwAbEOJqi1v3o78YwQAA\n",
              "AAwBn/l0Rf8AIKSVZWwAAAAOAZ/7akX/ACC+lR+MFC0AAAAaQZvgSahBbJlMCE///fEAAZF8CxHP\n",
              "KIvuGVEAAAAUQZ4eRRUsbwAbCRSipVxkMzh2iIAAAAAOAZ49dEX/ACCkpLie1iwAAAANAZ4/akX/\n",
              "ACC+g4bkjQAAABtBmiRJqEFsmUwIT//98QABkaDirHPsjP+1MGAAAAAcQZ5CRRUsbwAbGRoAEuTk\n",
              "Cul4ZCV3k4d1pOKnVQAAAA4BnmF0Rf8AIcNHJB4knAAAABsBnmNqRf8AIa4BAAnQ/Ghv0qFF+tJk\n",
              "RRSSTAkAAAAaQZpoSahBbJlMCE///fEAAZF8DGuZHL5q4ZUAAAAeQZ6GRRUsbwAbEOJ4Lws0gAWi\n",
              "3SF4giP7cK1bmeKBAAAADgGepXRF/wAhw+OgSuRtAAAAFwGep2pF/wAhq35+ABFczrJg07E5BNTM\n",
              "AAAAJEGarEmoQWyZTAhP//3xAAGRfAxrmVcskAFhxTn/z/UsdNwyoAAAACNBnspFFSxvABsZGgAb\n",
              "laH+FPbezoJainnpqCNLaUJNW4+DQQAAABoBnul0Rf8AIdeuABdBAjP0xDKLV9ByI3qZgAAAAA4B\n",
              "nutqRf8AIar3c1kd0AAAAElBmvBJqEFsmUwIT//98QABkXhZa4AWr6U7Y+k0juyjlZ1xKtj3bteZ\n",
              "CurZCYjsKu4c+qNA0aydFoA2GY2EEFsHxHk1esROtTBhAAAAH0GfDkUVLG8AGwtsnucGcyQALkLD\n",
              "u7E70ZSrGTh8dUEAAAAWAZ8tdEX/ACG3IEiAG5xYP7LpCPTvmQAAAAoBny9qRf8AAAYEAAAAO0Gb\n",
              "NEmoQWyZTAhP//3xAAGT3+JO0eqeysAPYUBPZncE1G9waTbxfa8wgrrPZFjjfsyr2VPQGdo1uTfg\n",
              "AAAAH0GfUkUVLG8AGv3g9w+SPTyAB/LJG8sUJAbEYycQuhEAAAARAZ9xdEX/ACG0vuNqn6Ko3zAA\n",
              "AAAQAZ9zakX/ACDK3TiJpnnV6QAAAClBm3hJqEFsmUwIT//98QAAybRBKYCwdACvovnzUb0XF342\n",
              "PrwD4FyTcQAAABRBn5ZFFSxvABpsmBl0om22SimmhAAAAA4Bn7V0Rf8AENuHzWmyRwAAAA8Bn7dq\n",
              "Rf8AEMOqeeIUW0EAAAAkQZu8SahBbJlMCE///fEAAMjWpJJuAG4DIFWHy6vfQJQouSbgAAAAE0Gf\n",
              "2kUVLG8ADXmnCBvhL0V0c5kAAAAbAZ/5dEX/ABDjTYACbsxUJ7sUI4M4J/9orZIwAAAADwGf+2pF\n",
              "/wAQy5TLK0yRgQAAABtBm+BJqEFsmUwIT//98QAAycAalQAuFAEZj2kAAAAfQZ4eRRUsbwANhoRr\n",
              "2QAfvLzP0ixYIus633GA4VyPDQAAABYBnj10Rf8AENuJPnAB/KqzLPTQLMkYAAAAEAGeP2pF/wAQ\n",
              "w6p5EXWCGSMAAAAaQZoiSahBbJlMFEwn//3xAADIvgclVtS5JuAAAAAaAZ5BakX/ABDmKkpoATTk\n",
              "2x9pxipGbK8KroEAAAArQZpGSeEKUmUwIT/98QAAyL0LiAAcbJqdrE15IPnf7LhruhThEU3/MZj2\n",
              "gAAAABNBnmRFNExvAA2CBsHAsXormW7BAAAAGQGeg3RF/wAQmcxOwAE6YHmO+HbyYmmNkjEAAAAP\n",
              "AZ6FakX/ABDmKW6ETJGBAAAAKkGaiUmoQWiZTAhP//3xAADIvg86AEluxEqGBmDa3x4hGYtEOCQ3\n",
              "r1EVMQAAABNBnqdFESxfABDjPg/pkYmO93rgAAAAGAGeyGpF/wAQ69cACaJv/IM/9FhFM6rGXAAA\n",
              "ADxBms1JqEFsmUwIT//98QAAyL0FjZkmSSO4AS2zFpQYpIN3RdQQaG1InJiCZT4IWLCGWpIDuiqi\n",
              "CwKz5u0AAAAeQZ7rRRUsbwANhbZUOEdOACsadvoL7hpBwfzlHPAtAAAAHAGfCnRF/wAQ4sNgA+3p\n",
              "J9G0FI+dSofH5+QiWBAAAAAQAZ8MakX/ABBV6ZVbXbBbQwAAADdBmxBJqEFsmUwIT//98QAAyL4F\n",
              "E0W2rqSxjyQIANVm7Ha8RrW/EP6B4+IfGAyXOe/HznH2tWDBAAAAFEGfLkUVLF8AEN8stl9eGgaX\n",
              "Eg+pAAAAFwGfT2pF/wAQzrCvu7aACGMw1k0AFLAgAAAAP0GbVEmoQWyZTAhH//3hAAE984EnCrsk\n",
              "uxDLMADZ2cG0fjJ3G7DBpvESbCCXCxttVqFkvUJG975ug8SpJ+cq/wAAACZBn3JFFSxvAA2CByQd\n",
              "7sMgAe7gT9R8HqRPUK06EmxBH4Jb0uL/kQAAAB0Bn5F0Rf8AEOUWkAC6Ev9zN6mNU0V9Xiz+j0JY\n",
              "EAAAABYBn5NqRf8AEOEbWABxgUkuLBGr6pgQAAAASkGbl0moQWyZTAhH//3hAAE//IHxJqA4wfyc\n",
              "IqfABao7//d4svcjcn0+XSymMK4IirL89TqfE45S5JsVmpwNJf1AGlob3NM/SW/BAAAAEkGftUUV\n",
              "LF8AENsQB4h3Ak9ZIAAAABoBn9ZqRf8AENXcxmAAhjWRXhwhHh1scXNmYQAAADVBm9lJqEFsmUwU\n",
              "TCP//eEAAJ6CwpCigAhOUU/2DobsukIi0UPjHO4rXlg35VsbTBdr+rLNSQAAAA4Bn/hqRf8ACCru\n",
              "imIaKAAAAEFBm/xJ4QpSZTAhH/3hAACfOkA4gAIWF9LTXBGT326DkBib0u3pKSpa7Ez9m0xuPLWk\n",
              "7vqdaG6+8VsYULCe/fMU3QAAACBBnhpFNExfABDfRBIkAEvTclFiXS9JHCKnueSqcVieEAAAAAwB\n",
              "njtqRf8AEOVsrVMAAAAqQZo9SahBaJlMCEf//eEAAJ7K/IAQc0kA7FkHYwhvOzGCQJAQ6dKBRlmp\n",
              "AAAAPUGaX0nhClJlMFESwn/98QAAyY/GHIh/AphQoxgA5onr74a+d/2yrp0VDfuqkuvYaaFeKSQh\n",
              "GHt10VlasGAAAAAfAZ5+akX/ABDVxgy7kgA2mFVuglv3rn2RqDN1ELU01AAAADxBmmFJ4Q6JlMFE\n",
              "wn/98QAAZIwzGAAC+ZuBleQP1ghBImXAZ2T0iymMtK9cN57RJV1m8+QmN/vwT+lzNSEAAAAOAZ6A\n",
              "akX/AAgq7opaPzgAAAA8QZqFSeEPJlMCE//98QAAZDMXljvy63ABzf0pSvaqGf/PN5Ngzq7C1GeP\n",
              "tmbPBIzBu3jpCf+TZ4TEFym5AAAAIEGeo0URPG8ABqvxtk5oAE5LssbyR2m1Xk4umjdtxFBCAAAA\n",
              "DQGewnRF/wAIKSSnvI0AAAANAZ7EakX/AAgq7bVORwAAAEBBmslJqEFomUwIT//98QAAyMqVccAe\n",
              "lc6sNFAALq9XYmI03xVFKPECS3rqe5r5qIXz0LXzQ87v27lwFMRoV98PAAAAIEGe50URLG8ABqu9\n",
              "g1d2HQAJThtlT0limletIeNirkE1AAAAGQGfBnRF/wAIdeuABAonowqKFlJJhl5NkTAAAAAfAZ8I\n",
              "akX/AAhrgEACF+k6Q/n6RWtxbkG1iCjDQBLlgAAAAD5Bmw1JqEFsmUwIR//94QAAn+2+4GwkgBCc\n",
              "nX/7vFl7kbk+nvV8OdRc10cXK6QO2CAxYs1kDx5GUGRydANm4QAAABhBnytFFSxvAA0vFRm/+LYr\n",
              "d7TEQEaZIyQAAAAcAZ9KdEX/AAhpciABDHRYzaqLgeHH7zVk46E4EAAAABkBn0xqRf8ACCr07nMA\n",
              "BXZvHHSQSXaIsV6FAAAAM0GbUUmoQWyZTAhH//3hAACf7b7ga24AHZDi86IegYrxKGLihx4IxbAs\n",
              "VZ0iNBstDI2bgQAAABZBn29FFSxvAAa7pDwUume2N5enxaFTAAAAGQGfjnRF/wAIKSk/GYACvd8h\n",
              "qAkTJAFAV6EAAAAXAZ+QakX/AAgg3DbTzmAAnTA7obWK9CAAAAAhQZuVSahBbJlMCL/6WAAEx66o\n",
              "cdo0eAE3dEuYA+SafoqTAAAAHEGfs0UVLG8ABr/ToAF0wcCOkkI4xrPGUsq0a0gAAAAaAZ/SdEX/\n",
              "AAgM2OgAnTeMwAE7ApLvy0vcubAAAAAcAZ/UakX/AAhq0AnxKQATp2/kEYEsDTwrqW0eBQAAC5Zt\n",
              "b292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAHHAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAA\n",
              "AAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAK\n",
              "wHRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAHHAAAAAAAAAAAAAAAAAAAAAAAAQAA\n",
              "AAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAABsAAAASAAAAAAACRlZHRzAAAAHGVsc3QA\n",
              "AAAAAAAAAQAABxwAAAEAAAEAAAAACjhtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADIAAABbAFXE\n",
              "AAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAnjbWluZgAA\n",
              "ABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAJ\n",
              "o3N0YmwAAACzc3RzZAAAAAAAAAABAAAAo2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAABsAEg\n",
              "AEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAxYXZj\n",
              "QwFkAB//4QAYZ2QAH6zZQbCWhAAAAwAEAAADAyA8YMZYAQAGaOvjyyLAAAAAHHV1aWRraEDyXyRP\n",
              "xbo5pRvPAyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAC2AAAAgAAAABRzdHNzAAAAAAAAAAEAAAAB\n",
              "AAAFoGN0dHMAAAAAAAAAsgAAAAEAAAEAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAAAAAAAQAAAIAA\n",
              "AAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAAAAABAAAAAAAA\n",
              "AAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAAAAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAA\n",
              "AQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAAAAABAAAAAAAAAAEAAACAAAAAAQAAAoAAAAAB\n",
              "AAABAAAAAAEAAAAAAAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEA\n",
              "AAKAAAAAAQAAAQAAAAABAAAAAAAAAAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAAAAAAAQAA\n",
              "AIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAAAAABAAAA\n",
              "AAAAAAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAAAAAAAQAAAIAAAAABAAACgAAAAAEAAAEA\n",
              "AAAAAQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAAAAABAAAAAAAAAAEAAACAAAAAAQAAAoAA\n",
              "AAABAAABAAAAAAEAAAAAAAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAA\n",
              "AAEAAAKAAAAAAQAAAQAAAAABAAAAAAAAAAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAAAAAA\n",
              "AQAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAAAAAB\n",
              "AAAAAAAAAAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAAAAAAAQAAAIAAAAABAAACgAAAAAEA\n",
              "AAEAAAAAAQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAAAAABAAAAAAAAAAEAAACAAAAAAQAA\n",
              "AoAAAAABAAABAAAAAAEAAAAAAAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAA\n",
              "gAAAAAEAAAKAAAAAAQAAAQAAAAABAAAAAAAAAAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAA\n",
              "AAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAA\n",
              "AAABAAAAAAAAAAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAAAAAAAQAAAIAAAAABAAACgAAA\n",
              "AAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEAAAGAAAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAA\n",
              "AQAAAAAAAAABAAAAgAAAAAEAAAIAAAAAAgAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAAB\n",
              "AAAAgAAAAAEAAAIAAAAAAgAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEA\n",
              "AAIAAAAAAgAAAIAAAAABAAABgAAAAAEAAACAAAAAAQAAAgAAAAACAAAAgAAAAAEAAAEAAAAAAQAA\n",
              "AYAAAAABAAAAgAAAAAEAAAGAAAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAA\n",
              "gAAAAAEAAAKAAAAAAQAAAQAAAAABAAAAAAAAAAEAAACAAAAAAQAAAoAAAAABAAABAAAAAAEAAAAA\n",
              "AAAAAQAAAIAAAAABAAACgAAAAAEAAAEAAAAAAQAAAAAAAAABAAAAgAAAAAEAAAKAAAAAAQAAAQAA\n",
              "AAABAAAAAAAAAAEAAACAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAAC2AAAAAQAAAuxzdHN6AAAAAAAA\n",
              "AAAAAAC2AAALNwAAAHQAAAAdAAAAFwAAABUAAAA7AAAAGAAAABQAAAARAAAAJQAAABUAAAAQAAAA\n",
              "EAAAABgAAAAUAAAAEAAAABAAAAA0AAAAFgAAABIAAAAQAAAANgAAABYAAAASAAAAEQAAACQAAAAW\n",
              "AAAAEQAAABAAAAAYAAAAFAAAABAAAAAQAAAALQAAABQAAAARAAAAEQAAADgAAAAgAAAAEQAAABEA\n",
              "AAAdAAAAFAAAABEAAAAOAAAANQAAAB4AAAAQAAAADgAAABwAAAAbAAAAEgAAABEAAAAcAAAAHAAA\n",
              "ABEAAAASAAAAHAAAABwAAAASAAAAEQAAACcAAAAeAAAADgAAABkAAAArAAAAFgAAABEAAAAQAAAA\n",
              "IAAAABUAAAAOAAAAEAAAAB8AAAAfAAAAEAAAABAAAAAdAAAAJgAAABMAAAAQAAAAMwAAABUAAAAQ\n",
              "AAAAEQAAACYAAAAgAAAAEAAAACIAAAAvAAAAFwAAABAAAAASAAAAHgAAABgAAAASAAAAEQAAAB8A\n",
              "AAAgAAAAEgAAAB8AAAAeAAAAIgAAABIAAAAbAAAAKAAAACcAAAAeAAAAEgAAAE0AAAAjAAAAGgAA\n",
              "AA4AAAA/AAAAIwAAABUAAAAUAAAALQAAABgAAAASAAAAEwAAACgAAAAXAAAAHwAAABMAAAAfAAAA\n",
              "IwAAABoAAAAUAAAAHgAAAB4AAAAvAAAAFwAAAB0AAAATAAAALgAAABcAAAAcAAAAQAAAACIAAAAg\n",
              "AAAAFAAAADsAAAAYAAAAGwAAAEMAAAAqAAAAIQAAABoAAABOAAAAFgAAAB4AAAA5AAAAEgAAAEUA\n",
              "AAAkAAAAEAAAAC4AAABBAAAAIwAAAEAAAAASAAAAQAAAACQAAAARAAAAEQAAAEQAAAAkAAAAHQAA\n",
              "ACMAAABCAAAAHAAAACAAAAAdAAAANwAAABoAAAAdAAAAGwAAACUAAAAgAAAAHgAAACAAAAAUc3Rj\n",
              "bwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBs\n",
              "AAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTcuODMuMTAw\n",
              "\">\n",
              "  Your browser does not support the video tag.\n",
              "</video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "render time: 9.587531 sec.\n",
            "---\n",
            "\n",
            "Game 1:\n",
            "Total reward: 8  FPS: 144\n",
            "Game 2:\n",
            "Total reward: 0  FPS: 153\n",
            "Game 3:\n",
            "Total reward: 30  FPS: 155\n",
            "Game 4:\n",
            "Total reward: 0  FPS: 152\n",
            "Game 5:\n",
            "Total reward: 4  FPS: 148\n",
            "Game 6:\n",
            "Total reward: 0  FPS: 150\n",
            "Game 7:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2dffba12d4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# обучим сеть по случайносу batch-у\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2dffba12d4c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# выгоды для конечных состояний\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# сформируем пустой обучающий пакет\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2dffba12d4c6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# предсказания сети по списку начальных состояний\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m#  предсказания сети по одном начальному состоянию\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}