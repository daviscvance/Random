{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of pybullet_ann.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daviscvance/Random/blob/master/pybullet_ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KEh7Oj2mh7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building wheels will take a couple minutes! Make sure to use the GPU instance\n",
        "!pip install pybullet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH0L5tglpAet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get -qq -y install ffmpeg > /dev/null\n",
        "# !ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBk03tTBoIN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random, numpy, math, time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import display, HTML\n",
        "import pybullet as pb\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import LSTM\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "\n",
        "####################\n",
        "# Environment (game)\n",
        "####################\n",
        "\n",
        "MAX_STEPS = 1000        # maximum number of simulation steps\n",
        "STEPS_AFTER_TARGET = 30 # number of simulation steps after reaching the goal\n",
        "TARGET_DELTA = 0.2      # value of acceptable rolling near the target (absolute value)\n",
        "FORCE_DELTA = 0.1       # step change of force (absolute value)\n",
        "PB_BallMass = 1         # ball weight\n",
        "PB_BallRadius = .2     # radius of the ball\n",
        "PB_HEIGHT = 10          # maximum height of raising the ball\n",
        "MAX_FORCE = 20          # maximum vertical force applied to the ball\n",
        "MIN_FORCE = 0           # minimum vertical force applied to the ball\n",
        "MAX_VEL = 14.2          # maximum vertical speed of the ball\n",
        "MIN_VEL = -14.2         # minimum vertical speed of the ball\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        # current state of environment\n",
        "        self.pb_z = 0               # current ball height\n",
        "        self.pb_force = 0           # current force applied to the ball\n",
        "        self.pb_velocity = 0        # current vertical speed of the ball\n",
        "        self.z_target = 0           # target height\n",
        "        self.start_time = 0         # start time of the new game\n",
        "        self.steps = 0              # number of steps after the start of the simulation\n",
        "        self.target_area = 0        # fact of reaching the target\n",
        "        self.steps_after_target = 0 # number of steps after reaching the goal\n",
        " \n",
        "        # create a simulation\n",
        "        self.pb_physicsClient = pb.connect(pb.DIRECT)\n",
        "\n",
        "    def reset(self):\n",
        "        # random height of the ball and target height\n",
        "        z_target = random.uniform(0.01, 0.99)\n",
        "        self.z_target = PB_BallRadius + z_target*PB_HEIGHT\n",
        "        z = random.uniform(0.05, 0.95)\n",
        "        self.pb_z = PB_BallRadius + z*PB_HEIGHT\n",
        "        \n",
        "        # reset of environmental parameters\n",
        "        pb.resetSimulation()\n",
        "        self.target_area = 0\n",
        "        self.start_time = time.time()\n",
        "        self.steps = 0\n",
        "        self.steps_after_target = 0\n",
        "        \n",
        "        # simulation step 1/60 sec..\n",
        "        pb.setTimeStep(1./60)\n",
        "        \n",
        "        # surface\n",
        "        floorColShape = pb.createCollisionShape(pb.GEOM_PLANE)\n",
        "        # for GEOM_PLANE, visualShape - not displayed, we will use GEOM_BOX\n",
        "        floorVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[100,100,0.0001], rgbaColor=[1,1,.98,1])\n",
        "        self.pb_floorId = pb.createMultiBody(0,floorColShape,floorVisualShapeId, [0,0,0], [0,0,0,1])# (mass,collisionShape,visualShape)\n",
        "        \n",
        "        # orb\n",
        "        ballPosition = [0,0,self.pb_z]\n",
        "        ballOrientation=[0,0,0,1]\n",
        "        ballColShape = pb.createCollisionShape(pb.GEOM_SPHERE,radius=PB_BallRadius)\n",
        "        ballVisualShapeId = pb.createVisualShape(pb.GEOM_SPHERE,radius=PB_BallRadius, rgbaColor=[0.25, 0.75, 0.25,1])\n",
        "        self.pb_ballId = pb.createMultiBody(PB_BallMass, ballColShape, ballVisualShapeId, ballPosition, ballOrientation) #(mass, collisionShape, visualShape, ballPosition, ballOrientation)\n",
        "        #pb.changeVisualShape(self.pb_ballId,-1,rgbaColor=[1,0.27,0,1])\n",
        "        \n",
        "        # target pointer (without CollisionShape, only display (VisualShape))\n",
        "        targetPosition = [0,0,self.z_target]\n",
        "        targetOrientation=[0,0,0,1]\n",
        "        targetVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[1,0.025,0.025], rgbaColor=[0,0,0,1])\n",
        "        self.pb_targetId = pb.createMultiBody(0,-1, targetVisualShapeId, targetPosition, targetOrientation)\n",
        "\n",
        "        # gravity\n",
        "        pb.setGravity(0,0,-10)\n",
        "\n",
        "        # limit the motion of the ball only along the vertical axes\n",
        "        pb.createConstraint(self.pb_floorId, -1, self.pb_ballId, -1, pb.JOINT_PRISMATIC, [0,0,1], [0,0,0], [0,0,0])\n",
        "\n",
        "        # set the acting force on the ball to compensate for gravity\n",
        "        self.pb_force = 10 * PB_BallMass\n",
        "        pb.applyExternalForce(self.pb_ballId, -1, [0,0,self.pb_force], [0,0,0], pb.LINK_FRAME)\n",
        "                \n",
        "        # return values\n",
        "        observation = self.getObservation()\n",
        "        reward, done = self.getReward()\n",
        "        info = self.getInfo()\n",
        "        return [observation, reward, done, info]\n",
        "\n",
        "    # Observations (return normalized state)\n",
        "    def getObservation(self):\n",
        "        # distance to target\n",
        "        d_target =  0.5 + (self.pb_z - self.z_target)/(2*PB_HEIGHT)\n",
        "        # acting force\n",
        "        force = (self.pb_force-MIN_FORCE)/(MAX_FORCE-MIN_FORCE)\n",
        "        # current ball height\n",
        "        z = (self.pb_z-PB_BallRadius)/PB_HEIGHT\n",
        "        # current speed\n",
        "        z_velocity = (self.pb_velocity-MIN_VEL)/(MAX_VEL-MIN_VEL)\n",
        "        state = [d_target, force, z_velocity]\n",
        "        return state\n",
        "\n",
        "    # reward per action calculation\n",
        "    def getReward(self):\n",
        "        done = False\n",
        "        z_reward = 0\n",
        "        # The fact of achieving the goal, then wait for the STEPS_AFTER_TARGET steps and complete the game.\n",
        "        if (TARGET_DELTA >= math.fabs(self.z_target - self.pb_z)):\n",
        "            self.target_area = 1\n",
        "            z_reward = 1\n",
        "        # Out of range\n",
        "        if (self.pb_z > (PB_HEIGHT + PB_BallRadius) or self.pb_z < PB_BallRadius):\n",
        "            done = True\n",
        "        # Completion of the game after reaching the goal\n",
        "        if (self.target_area > 0):\n",
        "            self.steps_after_target += 1\n",
        "            if (self.steps_after_target>=STEPS_AFTER_TARGET):\n",
        "                done = True\n",
        "        # Timeout game completion\n",
        "        if (self.steps >= MAX_STEPS):\n",
        "            done = True\n",
        "\n",
        "        return [z_reward, done]\n",
        "    \n",
        "    # Additional information for collecting statistics\n",
        "    def getInfo(self):\n",
        "        game_time = time.time() - self.start_time\n",
        "        if game_time:\n",
        "            fps = round(self.steps/game_time)\n",
        "        return {'step': self.steps, 'fps': fps}\n",
        "\n",
        "    # Run the simulation step according to the passed action\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        if action == 0:\n",
        "            # 0 - increase in applied force\n",
        "            self.pb_force -= FORCE_DELTA\n",
        "            if self.pb_force < MIN_FORCE:\n",
        "                self.pb_force = MIN_FORCE\n",
        "        else:\n",
        "            # 1 - reduction of applied force\n",
        "            self.pb_force += FORCE_DELTA\n",
        "            if self.pb_force > MAX_FORCE:\n",
        "                self.pb_force = MAX_FORCE\n",
        "        \n",
        "        # change the current forces and run the simulation step\n",
        "        pb.applyExternalForce(self.pb_ballId, -1, [0,0,self.pb_force], [0,0,0], pb.LINK_FRAME)\n",
        "        pb.stepSimulation()\n",
        "        \n",
        "        # update the environment state parameters (ball position and speed)\n",
        "        curPos, curOrient = pb.getBasePositionAndOrientation(self.pb_ballId)\n",
        "        lin_vel, ang_vel= pb.getBaseVelocity(self.pb_ballId)\n",
        "        self.pb_z = curPos[2]\n",
        "        self.pb_velocity = lin_vel[2]\n",
        "        \n",
        "        # we will return the observations, the reward, the fact of the end of the game and additional information\n",
        "        observation = self.getObservation()\n",
        "        reward, done = self.getReward()\n",
        "        info = self.getInfo()\n",
        "        return [observation, reward, done, info]\n",
        "    \n",
        "    # The current image from the camera\n",
        "    def render(self):\n",
        "        camTargetPos = [0,0,5] # target location (focus) of the camera\n",
        "        camDistance = 10       # camera distance from target\n",
        "        yaw = 0                # yaw angle relative to target\n",
        "        pitch = 0              # camera tilt relative to target\n",
        "        roll=0                 # camera roll angle relative to target\n",
        "        upAxisIndex = 2        # camera vertical axis (z)\n",
        "\n",
        "        fov = 60               # camera angle\n",
        "        nearPlane = 0.01       # distance to the near clipping plane\n",
        "        farPlane = 20          # distance to the distant cut plane\n",
        "        pixelWidth = 320       # image width\n",
        "        pixelHeight = 200      # image height\n",
        "        aspect = pixelWidth/pixelHeight;  # image aspect ratio\n",
        "       \n",
        "        # view matrix\n",
        "        viewMatrix = pb.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)\n",
        "        # projection matrix\n",
        "        projectionMatrix = pb.computeProjectionMatrixFOV(fov, aspect, nearPlane, farPlane);\n",
        "        # rendering camera images\n",
        "        img_arr = pb.getCameraImage(pixelWidth, pixelHeight, viewMatrix, \n",
        "                                    projectionMatrix, \n",
        "                                    shadow=0, \n",
        "                                    lightDirection=[0,1,1],\n",
        "                                    renderer=pb.ER_TINY_RENDERER)\n",
        "\n",
        "        w=img_arr[0]            # width of the image, in pixels\n",
        "        h=img_arr[1]            # height of the image, in pixels\n",
        "        rgb=img_arr[2]          # color data RGB\n",
        "        dep=img_arr[3]          # depth data\n",
        "        \n",
        "        # return rgb matrix\n",
        "        return rgb\n",
        "    \n",
        "#################################\n",
        "# Memory for teaching examples\n",
        "#################################\n",
        "\n",
        "MEMORY_CAPACITY = 200000\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.samples = []   # tuples of the type (s, a, r, s_) are stored\n",
        "\n",
        "    def add(self, sample):\n",
        "        self.samples.append(sample)\n",
        "        if len(self.samples) > MEMORY_CAPACITY:\n",
        "            self.samples.pop(0)\n",
        "\n",
        "    def sample(self, n):\n",
        "        n = min(n, len(self.samples))\n",
        "        return random.sample(self.samples, n)\n",
        "\n",
        "    \n",
        "##################\n",
        "# Neural network\n",
        "##################\n",
        "\n",
        "LAYER_SIZE = 512       # layer size\n",
        "STATE_CNT  = 3         # number of input parameters (distance to target + current force + speed)\n",
        "ACTION_CNT = 2         # number of outputs (reward for reducing and increasing strength)\n",
        "class Brain:\n",
        "    def __init__(self):\n",
        "        self.model = self._QNetwork()\n",
        "        \n",
        "    def _QNetwork(self):\n",
        "        # Create a network using Keras\n",
        "        model = Sequential()\n",
        "        model.add(Dense(units=LAYER_SIZE, activation='relu', input_dim=STATE_CNT))\n",
        "        model.add(Dense(units=LAYER_SIZE, activation='relu'))\n",
        "        model.add(Dense(units=ACTION_CNT, activation='linear'))\n",
        "        opt = RMSprop(lr=0.00025)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "        return model\n",
        "    \n",
        "    # learning by one package of teaching examples\n",
        "    def train(self, x, y, batch_size=32, epoch=1, verbose=0):\n",
        "        self.model.fit(x, y, batch_size=batch_size, epochs=epoch, verbose=verbose)\n",
        "\n",
        "    # network predictions from the list of initial states\n",
        "    def predict(self, s):\n",
        "        return self.model.predict(s)\n",
        "\n",
        "    #  network predictions for one initial state\n",
        "    def predictOne(self, s):\n",
        "        s = numpy.array(s)\n",
        "        predictions = self.predict(s.reshape(1, STATE_CNT)).flatten()\n",
        "        return predictions\n",
        "\n",
        "###############\n",
        "# Agent\n",
        "###############\n",
        "\n",
        "GAMMA = 0.98        # discount factor\n",
        "MAX_EPSILON = 0.5   # maximum probability of choosing a random action\n",
        "MIN_EPSILON = 0.1   # minimum probability of choosing a random action\n",
        "LAMBDA = 0.001      # parameter that determines the rate of decrease in the probability of choosing a random action\n",
        "BATCH_SIZE = 32     # training package size\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.brain = Brain()                     # Neural network for learning\n",
        "        self.memory = Memory()                   # Repository of case studies\n",
        "        self.epsilon = MAX_EPSILON               # Determines the probability of choosing a random action.\n",
        "\n",
        "    # choice of action\n",
        "    def act(self, s):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, ACTION_CNT - 1)        # choose a random action\n",
        "        else:\n",
        "            return numpy.argmax(self.brain.predictOne(s))   # we choose the optimal action\n",
        "        \n",
        "    # изменение состояния агента\n",
        "    def observe(self, sample, game_num):  # sample = (s, a, r, s_)\n",
        "        self.memory.add(sample)\n",
        "        self.epsilon = MIN_EPSILON + (MAX_EPSILON-MIN_EPSILON)*math.exp(-LAMBDA*game_num)\n",
        "\n",
        "    # training in a random batch of learning examples\n",
        "    def train(self):\n",
        "        batch = self.memory.sample(BATCH_SIZE)\n",
        "        batchLen = len(batch)  \n",
        "        if batchLen<BATCH_SIZE: # we will be trained only if there are enough examples in memory\n",
        "            return\n",
        "\n",
        "        # initial states from package\n",
        "        states = numpy.array([ o[0] for o in batch ])\n",
        "        # initial states from package\n",
        "        states_ = numpy.array([ o[3] for o in batch ])\n",
        "\n",
        "        # benefits for initial states\n",
        "        p = agent.brain.predict(states)\n",
        "        # выгоды для конечных состояний\n",
        "        p_ = agent.brain.predict(states_)\n",
        "\n",
        "        # сформируем пустой обучающий пакет\n",
        "        x = numpy.zeros((batchLen, STATE_CNT))\n",
        "        y = numpy.zeros((batchLen, ACTION_CNT))\n",
        "\n",
        "        # заполним пакет\n",
        "        for i in range(batchLen):\n",
        "            o = batch[i]\n",
        "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
        "\n",
        "            t = p[i] # выгоды действий для начального состояния\n",
        "            # обновим выгоду только для совершенного действия, для неиспользованных действий выгоды останутся прежними\n",
        "            t[a] = r + GAMMA * numpy.amax(p_[i]) # вычислим новую выгоду действия используя награду и максимальную выгоду конечного состояния\n",
        "            \n",
        "            # сохраним значения в batch\n",
        "            x[i] = s\n",
        "            y[i] = t\n",
        "\n",
        "        # обучим сеть по данному пакету\n",
        "        self.brain.train(x, y)\n",
        "\n",
        "#######################\n",
        "# Статистика\n",
        "#######################\n",
        "\n",
        "class Stats():\n",
        "    def __init__(self):\n",
        "        self.stats={\"game_num\": [],\"rewards\": [], \"success_steps\": [], \"fps\": [], \"steps\":[], \"epsilon\":[]}\n",
        "\n",
        "    def save_stat(self, R, info, epsilon, game_num):\n",
        "        self.stats[\"rewards\"].append(R)\n",
        "        self.stats[\"success_steps\"].append(R/STEPS_AFTER_TARGET)\n",
        "        self.stats[\"game_num\"].append(game_num)\n",
        "        self.stats[\"epsilon\"].append(epsilon)\n",
        "        self.stats[\"steps\"].append(info[\"step\"])\n",
        "        self.stats[\"fps\"].append(info[\"fps\"])\n",
        "    def show_stat(self):\n",
        "        # отобраим процент удачных шагов за опыт\n",
        "        plt.plot(self.stats[\"game_num\"], self.stats[\"success_steps\"], \"b.\")\n",
        "        # отобразим сглаженный график\n",
        "        x, y = self.fit_data(self.stats[\"game_num\"],  self.stats[\"success_steps\"])\n",
        "        plt.plot(x, y, \"r-\")\n",
        "        # второй вариант сглаживания    \n",
        "        # plt.plot(numpy.linspace(self.stats[\"game_num\"][0], self.stats[\"game_num\"][-1],50), numpy.average(numpy.array_split(self.stats[\"success_steps\"][:-1], 50),1), \"g-\")\n",
        "        plt.show()\n",
        "    #  Полиномиальное сглаживание\n",
        "    def fit_data(self, x, y):\n",
        "        z = numpy.polyfit(x, y, 3)\n",
        "        f = numpy.poly1d(z)\n",
        "        # новые данные размерностью 50\n",
        "        x_new = numpy.linspace(x[0], x[-1], 50)\n",
        "        y_new = f(x_new)\n",
        "        return [x_new, y_new]\n",
        "\n",
        "###########\n",
        "# MAIN\n",
        "###########\n",
        "%matplotlib inline\n",
        "MAX_GAMES = 50000   # максимальное количество игр\n",
        "RENDER_PERIOD = 100 # период генерации видео с опытом (0 для отключения)\n",
        "\n",
        "env = Environment()\n",
        "agent = Agent()\n",
        "stats = Stats()\n",
        "\n",
        "for game_num in range(MAX_GAMES):\n",
        "    print (\"Game %d:\" % game_num)\n",
        "    render_imgs = []\n",
        "    observation, r, done, info = env.reset()\n",
        "    s = observation\n",
        "    R = r\n",
        "    \n",
        "    if RENDER_PERIOD and (game_num % RENDER_PERIOD == 0):\n",
        "        plt.subplots()\n",
        "    \n",
        "    while True:\n",
        "        # возьмем оптимальное действие на основе текущего состояния\n",
        "        a = agent.act(s)\n",
        "        # запустим шаг симуляции\n",
        "        observation, r, done, info = env.step(a)\n",
        "        s_ = observation      # новое состояние\n",
        "        # сохраним состояние агента\n",
        "        agent.observe((s, a, r, s_), game_num)\n",
        "        # обучим сеть по случайносу batch-у\n",
        "        agent.train()\n",
        "        \n",
        "        s = s_\n",
        "        R += r\n",
        "        \n",
        "        # сохраним изображение, если необходимо\n",
        "        if RENDER_PERIOD and game_num % RENDER_PERIOD == 0:\n",
        "            rgb = env.render()\n",
        "            render_imgs.append([plt.imshow(rgb, animated=True)])\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        #time.sleep(1./130)\n",
        "\n",
        "    print(\"Total reward:\", R, \" FPS:\", info['fps'])\n",
        "    \n",
        "    # сохраним статистику\n",
        "    stats.save_stat(R, info, agent.epsilon, game_num)\n",
        "    \n",
        "    # сформируем анимацию игры и графики статистики обучения\n",
        "    if len(render_imgs):\n",
        "        render_start = time.time()\n",
        "        ani = animation.ArtistAnimation(plt.gcf(), render_imgs, interval=10, blit=True,repeat_delay=1000)\n",
        "        plt.close()\n",
        "        display(HTML(ani.to_html5_video()))\n",
        "        # статистика\n",
        "        if game_num != 0:\n",
        "            plt.subplots(figsize=(10,4))\n",
        "            stats.show_stat()\n",
        "            plt.close()\n",
        "        render_stop = time.time()\n",
        "        print (\"render time: %f sec.\\n---\\n\" % (render_stop - render_start))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}